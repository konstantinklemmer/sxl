{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example3_Spatial_Interpolation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ruInrq8ON3G",
        "colab_type": "text"
      },
      "source": [
        "# Example 3: Spatial Interpolation\n",
        "## Setup\n",
        "\n",
        "Install / load required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O67pgzbP22cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import requests\n",
        "from urllib.request import urlretrieve\n",
        "import urllib.request, json \n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from decimal import Decimal, getcontext\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT0l80BdONQk",
        "colab_type": "text"
      },
      "source": [
        "Check GPU device availability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjXTI_XrBQ9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R47BPj86OXnK",
        "colab_type": "text"
      },
      "source": [
        "Helper functions, as found in `src/utils.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu34YvsBBRYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalize data values. Default is [0,1] range; if min_val = -1, range is [-1,1]\n",
        "def normal(tensor,min_val=0):\n",
        "  t_min = torch.min(tensor)\n",
        "  t_max = torch.max(tensor)\n",
        "  if t_min == 0 and t_max == 0:\n",
        "    return torch.tensor(tensor)\n",
        "  if min_val == -1:\n",
        "    tensor_norm = 2 * ((tensor - t_min) / (t_max - t_min)) - 1\n",
        "  if min_val== 0:\n",
        "    tensor_norm = ((tensor - t_min) / (t_max - t_min))\n",
        "  return torch.tensor(tensor_norm)\n",
        "\n",
        "#Light-weight Local Moran's I for tensor data, requiring a sparse weight matrix input. \n",
        "#This can be used when there is no need to re-compute the weight matrix at each step\n",
        "def lw_tensor_local_moran(y,w_sparse,na_to_zero=True,norm=True,norm_min_val=0):\n",
        "  y = y.reshape(-1)\n",
        "  n = len(y)\n",
        "  n_1 = n - 1\n",
        "  z = y - y.mean()\n",
        "  sy = y.std()\n",
        "  z /= sy\n",
        "  den = (z * z).sum()\n",
        "  zl = torch.tensor(w_sparse * z)\n",
        "  mi = n_1 * z * zl / den\n",
        "  if na_to_zero==True:\n",
        "    mi[torch.isnan(mi)] = 0\n",
        "  if norm==True:\n",
        "    mi = normal(mi,min_val=norm_min_val)\n",
        "  return torch.tensor(mi)\n",
        "\n",
        "#Batch version of lw_tensor_local_moran\n",
        "#Computes the (normalized) local Moran's I for an input batch\n",
        "def batch_lw_tensor_local_moran(y_batch,w_sparse,na_to_zero=True,norm=True,norm_min_val=0):\n",
        "  batch_size = y_batch.shape[0]\n",
        "  N = y_batch.shape[3]\n",
        "  mi_y_batch = torch.zeros(y_batch.shape)\n",
        "  for i in range(batch_size):\n",
        "    y = y_batch[i,:,:,:].reshape(N,N)\n",
        "    y = y.reshape(-1)\n",
        "    n = len(y)\n",
        "    n_1 = n - 1\n",
        "    z = y - y.mean()\n",
        "    sy = y.std()\n",
        "    z /= sy\n",
        "    den = (z * z).sum()\n",
        "    zl = torch.tensor(w_sparse * z)\n",
        "    mi = n_1 * z * zl / den\n",
        "    if na_to_zero==True:\n",
        "      mi[torch.isnan(mi)] = 0\n",
        "    if norm==True:\n",
        "      mi = normal(mi,min_val=norm_min_val)\n",
        "    mi_y_batch[i,0,:,:] = mi.reshape(N,N)\n",
        "  return mi_y_batch    \n",
        "\n",
        "#Downsampling by average pooling (needed for computing the multi-res Moran's I)\n",
        "downsample = nn.AvgPool2d(kernel_size=2)\n",
        "\n",
        "#Compute batch RMSE\n",
        "def calc_rmse_batch(x,y):\n",
        "  mse = nn.MSELoss()\n",
        "  return torch.sqrt(mse(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVBw4mYVPCXA",
        "colab_type": "text"
      },
      "source": [
        "Load sparse spatial weight matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7NeOKB9_rIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_64.npz','w_sparse_64.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_32.npz','w_sparse_32.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_16.npz','w_sparse_16.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_8.npz','w_sparse_8.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_4.npz','w_sparse_4.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEdlCBb1mUM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w_sparse_64 = scipy.sparse.load_npz('w_sparse_64.npz')\n",
        "w_sparse_32 = scipy.sparse.load_npz('w_sparse_32.npz')\n",
        "w_sparse_16 = scipy.sparse.load_npz('w_sparse_16.npz')\n",
        "w_sparse_8 = scipy.sparse.load_npz('w_sparse_8.npz')\n",
        "w_sparse_4 = scipy.sparse.load_npz('w_sparse_4.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL5j8f86PFmd",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "\n",
        "Load and prepare the DEM data. Data is normalized in the range `[0,1]`. The local Moran's I of the data can be computed at this step already, to avoid further computational burden during training.\n",
        "\n",
        "Target (output) data is size `64 x 64`, inputs are either `32 x 32` or `16 x 16`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpkPiyd9BUFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = \"list_dtm.json\"\n",
        "with open(file_path, 'r') as j:\n",
        "  ys = np.array(json.loads(j.read()))\n",
        "\n",
        "file_path = \"list_dtm_small.json\"\n",
        "with open(file_path, 'r') as j:\n",
        "  xs_32 = np.array(json.loads(j.read()))\n",
        "\n",
        "file_path = \"list_dtm_small2.json\"\n",
        "with open(file_path, 'r') as j:\n",
        "  xs_16 = np.array(json.loads(j.read()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEOd5yJCCMAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "t = ys.shape[0]\n",
        "N1 = 64\n",
        "N2 = 32\n",
        "N3 = 16\n",
        "data_ys = torch.zeros(1674,4,N1,N1)\n",
        "data_xs_32 = torch.zeros(1674,2,N2,N2)\n",
        "data_xs_16 = torch.zeros(1674,2,N3,N3)\n",
        "for i in range(t-1):\n",
        "    train_x_32_t = torch.tensor(xs_32[i,:,:])\n",
        "    train_x_32_t = torch.tensor(StandardScaler().fit_transform(train_x_32_t.reshape(-1,1)))\n",
        "    train_x_32_t = torch.tensor(normal(train_x_32_t.reshape(-1),min_val=0))\n",
        "    train_x_16_t = torch.tensor(xs_16[i,:,:])\n",
        "    train_x_16_t = torch.tensor(StandardScaler().fit_transform(train_x_16_t.reshape(-1,1)))\n",
        "    train_x_16_t = torch.tensor(normal(train_x_16_t.reshape(-1),min_val=0))\n",
        "    train_y_t = torch.tensor(ys[i,:,:])\n",
        "    train_y_t = torch.tensor(StandardScaler().fit_transform(train_y_t.reshape(-1,1)))\n",
        "    train_y_t = torch.tensor(normal(train_y_t.reshape(-1),min_val=0))\n",
        "    data_xs_32[i,0,:,:] = train_x_32_t.reshape(N2,N2)\n",
        "    data_xs_16[i,0,:,:] = train_x_16_t.reshape(N3,N3)\n",
        "    data_ys[i,0,:,:] = train_y_t.reshape(N1,N1)\n",
        "data_xs_32[:,1,:,:] = batch_lw_tensor_local_moran(data_xs_32[:,0,:,:].reshape(t,1,N2,N2),w_sparse_32,norm_min_val=0).reshape(t,N2,N2)\n",
        "data_xs_16[:,1,:,:] = batch_lw_tensor_local_moran(data_xs_16[:,0,:,:].reshape(t,1,N3,N3),w_sparse_16,norm_min_val=0).reshape(t,N3,N3)\n",
        "data_ys[:,1,:,:] = batch_lw_tensor_local_moran(data_ys[:,0,:,:].reshape(t,1,N1,N1),w_sparse_64,norm_min_val=0).reshape(t,N1,N1)\n",
        "\n",
        "data_ys_d1 = downsample(data_ys[:,0,:,:].reshape(t,1,N1,N1))\n",
        "data_ys_d2 = downsample(data_ys_d1)\n",
        "data_mi_ys_d1 = batch_lw_tensor_local_moran(data_ys_d1[:,0,:,:].reshape(t,1,N1//2,N1//2),w_sparse_32,norm_min_val=0)\n",
        "data_mi_ys_d2 = batch_lw_tensor_local_moran(data_ys_d2[:,0,:,:].reshape(t,1,N1//4,N1//4),w_sparse_16,norm_min_val=0)\n",
        "data_ys[:,2,:,:] = nn.functional.interpolate(data_mi_ys_d1,scale_factor=2,mode=\"nearest\").reshape(t,N1,N1)\n",
        "data_ys[:,3,:,:] = nn.functional.interpolate(data_mi_ys_d2,scale_factor=4,mode=\"nearest\").reshape(t,N1,N1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38uzy2KQPhnh",
        "colab_type": "text"
      },
      "source": [
        "## Training \n",
        "\n",
        "Define the model architectures the different CNN models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOWB-W1nT8jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define possible models\n",
        "\n",
        "class CNN_32(nn.Module):\n",
        "    \"\"\"\n",
        "        CNN\n",
        "    \"\"\"\n",
        "    def __init__(self,  nc=1, ngf=32):\n",
        "        super(CNN_32, self).__init__()\n",
        "        #assert INPUT_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (INPUT_DIM[0] // 2**4, INPUT_DIM[1] // 2**4)\n",
        "        self.conv_net = nn.Sequential(nn.ConvTranspose2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(ngf*2,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        y_ = x.view(x.size(0), 1, 32, 32)\n",
        "        y_ = self.conv_net(y_)\n",
        "        return y_\n",
        "\n",
        "class CNN_MAT_32(nn.Module):\n",
        "    \"\"\"\n",
        "        CNN w/ Moran's Auxiliary Task\n",
        "    \"\"\"\n",
        "    def __init__(self,  nc=1, ngf=32):\n",
        "        super(CNN_MAT_32, self).__init__()\n",
        "        #assert INPUT_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (INPUT_DIM[0] // 2**4, INPUT_DIM[1] // 2**4)\n",
        "        self.conv_net = nn.Sequential(nn.ConvTranspose2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(nn.ConvTranspose2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(ngf*2,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )\n",
        "        self.output_t2 = nn.Sequential(nn.ConvTranspose2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(ngf*2,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_32)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        y_ = x.view(x.size(0), 1, 32, 32)\n",
        "        mi_y_ = mi_x.view(mi_x.size(0), 1, 32, 32)\n",
        "        y_ = self.conv_net(y_)\n",
        "        mi_y_ = self.conv_net(mi_y_)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        return y_, mi_y_\n",
        "\n",
        "class CNN_16(nn.Module):\n",
        "    \"\"\"\n",
        "        CNN\n",
        "    \"\"\"\n",
        "    def __init__(self,  nc=1, ngf=16):\n",
        "        super(CNN_16, self).__init__()\n",
        "        #assert INPUT_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (INPUT_DIM[0] // 2**4, INPUT_DIM[1] // 2**4)\n",
        "        self.conv_net = nn.Sequential(nn.ConvTranspose2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        y_ = x.view(x.size(0), 1, 16, 16)\n",
        "        y_ = self.conv_net(y_)\n",
        "        return y_\n",
        "\n",
        "class CNN_MAT_16(nn.Module):\n",
        "    \"\"\"\n",
        "        CNN\n",
        "    \"\"\"\n",
        "    def __init__(self,  nc=1, ngf=16):\n",
        "        super(CNN_MAT_16, self).__init__()\n",
        "        #assert INPUT_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (INPUT_DIM[0] // 2**4, INPUT_DIM[1] // 2**4)\n",
        "        self.conv_net = nn.Sequential(nn.ConvTranspose2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )\n",
        "        self.output_t2 = nn.Sequential(nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_16)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        y_ = x.view(x.size(0), 1, 16, 16)\n",
        "        mi_y_ = mi_x.view(mi_x.size(0), 1, 16, 16)\n",
        "        y_ = self.conv_net(y_)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.conv_net(mi_y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        return y_, mi_y_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRh8T843P412",
        "colab_type": "text"
      },
      "source": [
        "Define training configuration:\n",
        "\n",
        "- `input_size`: Interpolation input size; either 16 or 32\n",
        "- `model`: chose model; either Vanilla \"CNN\" or \"CNN_MAT\"\n",
        "- `lambda_`: Auxiliary task weight $\\lambda$\n",
        "- `num_epochs`: number of training epochs \n",
        "- `batch_size`: training batch size\n",
        "- `loss_fun`: loss function to be used. Default is MSE loss, but L1 or SmoothL1 can be used too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdyWjk_watC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### DEFINE EXPERIMENT SETTINGS ###\n",
        "input_size = \"32\" #Options are [\"16\",\"32\"]\n",
        "model = \"CNN\" # Options are [\"CNN\",\"CNN_MAT\"]\n",
        "lambda_ = 0.1 \n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Train on GPU or CPU\n",
        "loss_fun = nn.MSELoss() #Other options include e.g. nn.L1Loss() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtvnKv8AUKUX",
        "colab_type": "text"
      },
      "source": [
        "Define training loop and train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY_hZR1qd2ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initiate data and prepare train / val / test split\n",
        "N = int(input_size)\n",
        "train_ys = data_ys[:1000,:,:,:] \n",
        "val_ys = data_ys[1000:1300,:,:,:] \n",
        "test_ys = data_ys[1300:,:,:,:] \n",
        "if input_size==\"16\":\n",
        "  train_xs = data_xs_16[:1000,:,:,:] \n",
        "  val_xs = data_xs_16[1000:1300,:,:,:] \n",
        "  test_xs = data_xs_16[1300:,:,:,:] \n",
        "if input_size==\"32\":\n",
        "  train_xs = data_xs_32[:1000,:,:,:] \n",
        "  val_xs = data_xs_32[1000:1300,:,:,:] \n",
        "  test_xs = data_xs_32[1300:,:,:,:] \n",
        "#Prepare data loaders\n",
        "train_data = TensorDataset(train_ys, train_xs)\n",
        "val_data = TensorDataset(val_ys, val_xs)\n",
        "test_data = TensorDataset(test_ys, test_xs)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "#Load models\n",
        "if input_size==\"16\":\n",
        "  if model==\"CNN\":\n",
        "    Net = CNN_16().to(DEVICE)\n",
        "  elif model==\"CNN_MAT\":\n",
        "    Net = CNN_MAT_16().to(DEVICE)\n",
        "if input_size==\"32\":\n",
        "  if model==\"CNN\":\n",
        "    Net = CNN_32().to(DEVICE)\n",
        "  elif model==\"CNN_MAT\":\n",
        "    Net = CNN_MAT_32().to(DEVICE)\n",
        "#Utilities\n",
        "rmse_val = []\n",
        "check_step =[]\n",
        "criterion = loss_fun\n",
        "step = 0\n",
        "#Initiate optimizer\n",
        "Net_opt = torch.optim.Adam(Net.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "###TRAINING\n",
        "print(\"EXPERIMENT: \" + str(input_size) + \" -> 64 - MODEL: \" + model + \" LAMBDA: \" + str(lambda_))\n",
        "if model==\"CNN\":\n",
        "  for e in range(num_epochs):\n",
        "    # Within each iteration, we will go over each minibatch of data\n",
        "    for minibatch_i, (y_batch,x_batch) in enumerate(train_loader):\n",
        "      # Get data\n",
        "      x = x_batch.to(DEVICE)\n",
        "      y = y_batch.to(DEVICE)\n",
        "      mi_x = x[:,1,:,:].reshape(batch_size,1,N,N)\n",
        "      x = x[:,0,:,:].reshape(batch_size,1,N,N)\n",
        "      mi_y = y[:,1,:,:].reshape(batch_size,1,64,64)\n",
        "      y = y[:,0,:,:].reshape(batch_size,1,64,64)\n",
        "      # Training Net\n",
        "      x_outputs = Net(x)\n",
        "      Net_loss = criterion(x_outputs, y)\n",
        "      Net.zero_grad()\n",
        "      Net_loss.backward()\n",
        "      Net_opt.step()\n",
        "      # Model selection every 100 training steps\n",
        "      if step % 100 == 0:\n",
        "        check_step.append(step)\n",
        "        torch.save(Net, \"model_iter %d.pkl.gz\" % step)\n",
        "        rmse_batch = []\n",
        "        with torch.no_grad():\n",
        "            for (y_test, x_test) in val_loader:\n",
        "                y_test = y_test.to(DEVICE)\n",
        "                y_test = y_test[:,0,:,:].reshape(batch_size,1,64,64)\n",
        "                outputs = Net(x_test[:,0,:,:].to(DEVICE).reshape(batch_size,1,N,N))\n",
        "                rmse_ = calc_rmse_batch(outputs,y_test)\n",
        "                rmse_batch.append(rmse_)\n",
        "        rmse_val.append(torch.mean(torch.tensor(rmse_batch)))\n",
        "        #Print progress\n",
        "        print('Epoch [%d/%d] - Loss: %f' % (e, num_epochs, Net_loss.item()))\n",
        "      #Increment steps\n",
        "      step = step + 1\n",
        "    #Save best and full model:\n",
        "    tmp, idx = torch.min(torch.tensor(rmse_val),0)\n",
        "    idx = check_step[idx]\n",
        "    best_net = torch.load(\"model_iter \" + str(idx) + \".pkl.gz\")\n",
        "\n",
        "# CNN + MAT\n",
        "if model==\"CNN_MAT\":\n",
        "  for e in range(num_epochs):\n",
        "    # Within each iteration, we will go over each minibatch of data\n",
        "    for minibatch_i, (y_batch,x_batch) in enumerate(train_loader):\n",
        "      # Get data\n",
        "      x = x_batch.to(DEVICE)\n",
        "      y = y_batch.to(DEVICE)\n",
        "      x = x[:,0,:,:].reshape(batch_size,1,N,N)\n",
        "      mi_y = y[:,1,:,:].reshape(batch_size,1,64,64)\n",
        "      y = y[:,0,:,:].reshape(batch_size,1,64,64)\n",
        "      # Training Net\n",
        "      x_outputs, mi_x_outputs = Net(x)\n",
        "      Net_x_loss = criterion(x_outputs, y)\n",
        "      Net_mi_x_loss = criterion(mi_x_outputs, mi_y)\n",
        "      Net_loss = Net_x_loss + lambda_ * (Net_mi_x_loss)\n",
        "      Net.zero_grad()\n",
        "      Net_loss.backward()\n",
        "      Net_opt.step()\n",
        "      #Save model\n",
        "      if step % 100 == 0:\n",
        "        check_step.append(step)\n",
        "        torch.save(Net, \"model_iter %d.pkl.gz\" % step)\n",
        "        rmse_batch = []\n",
        "        with torch.no_grad():\n",
        "            for (y_test, x_test) in val_loader:\n",
        "                y_test = y_test.to(DEVICE)\n",
        "                y_test = y_test[:,0,:,:].reshape(batch_size,1,64,64)\n",
        "                outputs, _ = Net(x_test[:,0,:,:].to(DEVICE).reshape(batch_size,1,N,N))\n",
        "                rmse_ = calc_rmse_batch(outputs,y_test)\n",
        "                rmse_batch.append(rmse_)\n",
        "        rmse_val.append(torch.mean(torch.tensor(rmse_batch)))\n",
        "        #Print progress\n",
        "        print('Epoch [%d/%d] - Loss: %f' % (e, num_epochs, Net_loss.item()))\n",
        "      #Increment steps\n",
        "      step = step + 1\n",
        "  #Save best and full model:\n",
        "  tmp, idx = torch.min(torch.tensor(rmse_val),0)\n",
        "  idx = check_step[idx]\n",
        "  best_net = torch.load(\"model_iter \" + str(idx) + \".pkl.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v_9t1ZxUlR3",
        "colab_type": "text"
      },
      "source": [
        "Compute test scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYNdvlLtUGy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for (y_test, x_test) in test_loader:\n",
        "    # Get data\n",
        "    x = x_test.to(DEVICE)\n",
        "    y = y_test.to(DEVICE)\n",
        "    mi_x = x[:,1,:,:].reshape(batch_size,1,N,N)\n",
        "    x = x[:,0,:,:].reshape(batch_size,1,N,N)\n",
        "    mi_y = y[:,1,:,:].reshape(batch_size,1,64,64)\n",
        "    y = y[:,0,:,:].reshape(batch_size,1,64,64)\n",
        "    # Training Net\n",
        "    if model==\"CNN\":\n",
        "      x_outputs = best_net(x)\n",
        "    if model==\"CNN_MAT\":\n",
        "      x_outputs, _ = best_net(x)\n",
        "    rmse = calc_rmse_batch(x_outputs,y)\n",
        "    rmse_scores.append(rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JHHwGVgVHcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse_mean = torch.mean(torch.tensor(rmse_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXsXXyXtVI2a",
        "colab_type": "text"
      },
      "source": [
        "Print the final RMSE score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWaf2m-bVKpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse_mean"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}