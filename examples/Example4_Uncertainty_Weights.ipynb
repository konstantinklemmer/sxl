{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example4_Uncertainty_Weights.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm2vebW3Y5RL"
      },
      "source": [
        "# Example 4: Task uncertainty for auxiliary loss weighting\n",
        "## Setup\n",
        "\n",
        "Install / load required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYDZP7iivcYG"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import requests\n",
        "from urllib.request import urlretrieve\n",
        "import urllib.request, json \n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from decimal import Decimal, getcontext\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn import metrics"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iJHIax0UnG6"
      },
      "source": [
        "Check GPU device availability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOXRv5W6UpKQ"
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTAJRx7ueNwF"
      },
      "source": [
        "Helper functions, as found in `src/utils.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQWa9wRfv1xt"
      },
      "source": [
        "#Normalize data values. Default is [0,1] range; if min_val = -1, range is [-1,1]\n",
        "def normal(tensor,min_val=-1):\n",
        "  t_min = torch.min(tensor)\n",
        "  t_max = torch.max(tensor)\n",
        "  if t_min == 0 and t_max == 0:\n",
        "    return torch.tensor(tensor)\n",
        "  if min_val == -1:\n",
        "    tensor_norm = 2 * ((tensor - t_min) / (t_max - t_min)) - 1\n",
        "  if min_val== 0:\n",
        "    tensor_norm = ((tensor - t_min) / (t_max - t_min))\n",
        "  return torch.tensor(tensor_norm)\n",
        "\n",
        "#Light-weight Local Moran's I for tensor data, requiring a sparse weight matrix input. \n",
        "#This can be used when there is no need to re-compute the weight matrix at each step\n",
        "def lw_tensor_local_moran(y,w_sparse,na_to_zero=True,norm=True,norm_min_val=-1):\n",
        "  y = y.reshape(-1)\n",
        "  n = len(y)\n",
        "  n_1 = n - 1\n",
        "  z = y - y.mean()\n",
        "  sy = y.std()\n",
        "  z /= sy\n",
        "  den = (z * z).sum()\n",
        "  zl = torch.tensor(w_sparse * z)\n",
        "  mi = n_1 * z * zl / den\n",
        "  if na_to_zero==True:\n",
        "    mi[torch.isnan(mi)] = 0\n",
        "  if norm==True:\n",
        "    mi = normal(mi,min_val=norm_min_val)\n",
        "  return torch.tensor(mi)\n",
        "\n",
        "#Batch version of lw_tensor_local_moran\n",
        "#Computes the (normalized) local Moran's I for an input batch\n",
        "def batch_lw_tensor_local_moran(y_batch,w_sparse,na_to_zero=True,norm=True,norm_min_val=-1):\n",
        "  batch_size = y_batch.shape[0]\n",
        "  N = y_batch.shape[3]\n",
        "  mi_y_batch = torch.zeros(y_batch.shape)\n",
        "  for i in range(batch_size):\n",
        "    y = y_batch[i,:,:,:].reshape(N,N)\n",
        "    y = y.reshape(-1)\n",
        "    n = len(y)\n",
        "    n_1 = n - 1\n",
        "    z = y - y.mean()\n",
        "    sy = y.std()\n",
        "    z /= sy\n",
        "    den = (z * z).sum()\n",
        "    zl = torch.tensor(w_sparse * z)\n",
        "    mi = n_1 * z * zl / den\n",
        "    if na_to_zero==True:\n",
        "      mi[torch.isnan(mi)] = 0\n",
        "    if norm==True:\n",
        "      mi = normal(mi,min_val=norm_min_val)\n",
        "    mi_y_batch[i,0,:,:] = mi.reshape(N,N)\n",
        "  return mi_y_batch    \n",
        "\n",
        "#Downsampling by average pooling (needed for computing the multi-res Moran's I)\n",
        "downsample = nn.AvgPool2d(kernel_size=2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N80J5ZKeybo"
      },
      "source": [
        "Load sparse spatial weight matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52YHhB1s_oqA"
      },
      "source": [
        "%%capture\n",
        "\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_64.npz','w_sparse_64.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_32.npz','w_sparse_32.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_16.npz','w_sparse_16.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_8.npz','w_sparse_8.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_4.npz','w_sparse_4.npz')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iEDEtRUmzTb"
      },
      "source": [
        "w_sparse_64 = scipy.sparse.load_npz('w_sparse_64.npz')\n",
        "w_sparse_32 = scipy.sparse.load_npz('w_sparse_32.npz')\n",
        "w_sparse_16 = scipy.sparse.load_npz('w_sparse_16.npz')\n",
        "w_sparse_8 = scipy.sparse.load_npz('w_sparse_8.npz')\n",
        "w_sparse_4 = scipy.sparse.load_npz('w_sparse_4.npz')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDwt8YDd0IZz"
      },
      "source": [
        "##Data\n",
        "\n",
        "As customary for GAN training, data is normalized in the range `[-1,1]`. The local Moran's I of the data can be computed at this step already, to avoid further computational burden during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14VHz1xQzFh"
      },
      "source": [
        "### Petrel grid (32x32)\n",
        "\n",
        "Download and prepare data. We will work with the *PetrelGrid* dataset here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "149_fUvcRFEy"
      },
      "source": [
        "with urllib.request.urlopen(\"https://github.com/konstantinklemmer/sxl/raw/master/data/list_petrel.json\") as url:\n",
        "    train_y = np.array(json.loads(url.read().decode()))\n",
        "\n",
        "N = 32\n",
        "t = train_y.shape[0]\n",
        "data = torch.zeros(t,2,N,N)\n",
        "for i in range(t-1):\n",
        "    train_y_t = torch.tensor(train_y[i,:,:])\n",
        "    train_y_t = torch.tensor(normal(train_y_t.reshape(-1)))\n",
        "    data[i,0,:,:] = train_y_t.reshape(N,N)\n",
        "data[:,1,:,:] = batch_lw_tensor_local_moran(data[:,0,:,:].reshape(t,1,N,N),w_sparse_32,norm_min_val=-1).reshape(t,N,N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4HuPIwhh5m1"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE4gSMROikB4"
      },
      "source": [
        "Define the model architectures for Discriminator (**D**) and Generator (**G**). In this examples we use a EDGAN with MRES-MAT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmGUL84Q1fWC"
      },
      "source": [
        "###\n",
        "# EDGAN\n",
        "###\n",
        "\n",
        "class Discriminator_EDGAN_MRES_MAT_32(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Discriminator\n",
        "    \"\"\"\n",
        "    def __init__(self,nc=1,ndf1=32):\n",
        "        super(Discriminator_EDGAN_MRES_MAT_32,self).__init__()\n",
        "        self.conv = nn.Sequential(nn.Conv2d(nc,ndf1,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf1),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ndf1,ndf1*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf1*2),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ndf1*2,ndf1*4,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf1*4),\n",
        "          nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(nn.Conv2d(ndf1*4,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "        self.output_t2 = nn.Sequential(nn.Conv2d(ndf1*4,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "        self.output_t3 = nn.Sequential(nn.Conv2d(ndf1*4,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "        self.output_t4 = nn.Sequential(nn.Conv2d(ndf1*4,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        x_d1 = downsample(x)\n",
        "        x_d2 = downsample(x_d1)\n",
        "        mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_32)\n",
        "        mi_x_d1 = batch_lw_tensor_local_moran(x_d1.detach().cpu(),w_sparse_16)\n",
        "        mi_x_d2 = batch_lw_tensor_local_moran(x_d2.detach().cpu(),w_sparse_8)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        mi_x_d1 = mi_x_d1.to(DEVICE)\n",
        "        mi_x_d2 = mi_x_d2.to(DEVICE)\n",
        "        mi_x_d1 = nn.functional.interpolate(mi_x_d1,scale_factor=2,mode=\"nearest\")\n",
        "        mi_x_d2 = nn.functional.interpolate(mi_x_d2,scale_factor=4,mode=\"nearest\")\n",
        "        y_ = self.conv(x)\n",
        "        mi_y_ = self.conv(mi_x)\n",
        "        mi_y_d1 = self.conv(mi_x_d1)\n",
        "        mi_y_d2 = self.conv(mi_x_d2)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        mi_y_d1 = self.output_t3(mi_y_d1)\n",
        "        mi_y_d2 = self.output_t4(mi_y_d2)\n",
        "        y_ = y_.view(y_.size(0), -1)\n",
        "        mi_y_ = mi_y_.view(mi_y_.size(0), -1)\n",
        "        mi_y_d1 = mi_y_d1.view(mi_y_d1.size(0), -1)\n",
        "        mi_y_d2 = mi_y_d2.view(mi_y_d2.size(0), -1)\n",
        "        return y_, mi_y_, mi_y_d1, mi_y_d2\n",
        "\n",
        "class Generator_EDGAN_32(nn.Module):\n",
        "    \"\"\"\n",
        "        Encoder-Decoder Generator\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=100, nc=1, ngf=N):\n",
        "        super(Generator_EDGAN_32, self).__init__()\n",
        "        assert IMAGE_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (IMAGE_DIM[0] // 2**4, IMAGE_DIM[1] // 2**4)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, N*N),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.encoder = nn.Sequential(nn.Conv2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ngf*2,ngf*4,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*4),\n",
        "          nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(nn.ConvTranspose2d(ngf*4,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf*2,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )   \n",
        "    def forward(self, x, y=None):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_ = self.fc(x)\n",
        "        y_ = y_.view(y_.size(0), 1, N, N)\n",
        "        y_ = self.encoder(y_)\n",
        "        y_ = self.decoder(y_)\n",
        "        return y_"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tjuXChtrfjZ"
      },
      "source": [
        "Define Multi-Task loss wrapper (code adapted from: https://github.com/Hui-Li/multi-task-learning-example-PyTorch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADtaXhi-riV9"
      },
      "source": [
        "class MultiTaskLoss_Discriminator(nn.Module):\n",
        "    def __init__(self, task_num, model):\n",
        "        super(MultiTaskLoss_Discriminator, self).__init__()\n",
        "        self.model = model\n",
        "        self.task_num = task_num\n",
        "        self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
        "\n",
        "    def forward(self, input, targets):\n",
        "\n",
        "        outputs = self.model(input)\n",
        "\n",
        "        precision1 = 1 * torch.exp(-self.log_vars[0])\n",
        "        loss1 = criterion(outputs[0],targets[0])\n",
        "        loss1 = torch.sum(precision1 * (targets[0] - outputs[0]) ** 2. + self.log_vars[0], -1)\n",
        "\n",
        "        precision2 = 1 * torch.exp(-self.log_vars[1])\n",
        "        loss2 = criterion(outputs[1],targets[1])\n",
        "        loss2 = torch.sum(precision2 * (targets[1] - outputs[1]) ** 2. + self.log_vars[1], -1)\n",
        "\n",
        "        precision3 = 1 * torch.exp(-self.log_vars[2])\n",
        "        loss3 = criterion(outputs[2],targets[2])\n",
        "        loss3 = torch.sum(precision3 * (targets[2] - outputs[2]) ** 2. + self.log_vars[2], -1)\n",
        "\n",
        "        precision4 = 1 * torch.exp(-self.log_vars[3])\n",
        "        loss4 = criterion(outputs[3],targets[3])\n",
        "        loss4 = torch.sum(precision4 * (targets[3] - outputs[3]) ** 2. + self.log_vars[3], -1)\n",
        "\n",
        "        loss = loss1 + loss2 + loss3 + loss4\n",
        "        loss = torch.mean(loss)\n",
        "        return loss, self.log_vars.data.tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhl3VFqmrF_r"
      },
      "source": [
        "Define training configuration:\n",
        "\n",
        "- `train_split`: % of data to use for training (in case held-out data is needed for evaluation)\n",
        "- `batch_size`: training batch size\n",
        "- `num_epochs`: number of training epochs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuEewWsTrJgq"
      },
      "source": [
        "getcontext().prec = 3\n",
        "torch.manual_seed(99)\n",
        "\n",
        "### DEFINE EXPERIMENT SETTINGS ###\n",
        "train_split = Decimal(0.8) # 80% training data\n",
        "batch_size = 32 # define batch size\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Train on GPU or CPU\n",
        "num_epochs = 500 # Number of training epochs\n",
        "###"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAOTwK_ZKI-m"
      },
      "source": [
        "Define the training loop and train the model! We learn the uncertainty $\\sigma_{i}$ of task $i$ by tweaking the Discriminator loss function, so that the min-max game between Generator and Discriminator is given as:\n",
        "\n",
        "$$\\min_G \\max_D \\mathcal{L}_{MRES-MAT-UW} (D,G) = \\mathcal{L}_{GAN} (D,G) + \\\\\n",
        "(1 / 2 \\sigma_{1}^{2} \\mathcal{L}_{T_{1}}^{(D)} + \\dots + 1 / 2 \\sigma_{N}^{2} \\mathcal{L}_{T_{N}}^{(D)} + \\sum_{i=1}^{N} \\log \\sigma_i )$$ \n",
        "\n",
        "The task uncertainties $\\sigma_{i}$ define the contribution of each task $T_{i}$ (including main task!) to the composite loss. The `MultiTaskLoss_Discriminator` class allows us to update these uncertainties throughout training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MT8AmENnkq1"
      },
      "source": [
        "#Prepare input\n",
        "test_split = Decimal(1 - train_split)\n",
        "n = data.shape[0]\n",
        "N = data.shape[3]\n",
        "IMAGE_DIM = (N,N,1)\n",
        "train_set, test_set = torch.utils.data.random_split(data, [int(n * train_split), int(n * test_split)])\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "#Set Discriminator and Generator\n",
        "D = Discriminator_EDGAN_MRES_MAT_32().to(DEVICE)\n",
        "G = Generator_EDGAN_32().to(DEVICE)\n",
        "#Prepare labels\n",
        "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator Label to real\n",
        "D_labels = D_labels - 0.1 #This can be skipped\n",
        "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label to fake\n",
        "#Prepare training\n",
        "criterion = nn.BCELoss() #Binary cross entropy loss\n",
        "#Initiate opzimizer\n",
        "mtl_D = MultiTaskLoss_Discriminator(4, D).to(DEVICE)\n",
        "mtl_D_opt = torch.optim.Adam(mtl_D.parameters(),lr = 0.001, betas=(0.5, 0.999))\n",
        "G_opt = torch.optim.Adam(G.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "# Utilities\n",
        "step = 0\n",
        "n_noise = 100\n",
        "loss_d = []\n",
        "loss_g = []\n",
        "log_var1 = []\n",
        "log_var2 = []\n",
        "log_var3 = []\n",
        "log_var4 = []\n",
        "### TRAINING\n",
        "for e in range(num_epochs):\n",
        "    # Within each iteration, we will go over each minibatch of data\n",
        "    for minibatch_i, (x_batch) in enumerate(train_loader):\n",
        "      # Get data\n",
        "      x = x_batch[:,0,:,:]\n",
        "      x = x.reshape(batch_size,1,N,N).to(DEVICE)\n",
        "      ### Training Discriminator\n",
        "      z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "      z_gen = G(z)\n",
        "      D_x_loss, log_vars = mtl_D(x, [D_labels,D_labels,D_labels,D_labels])\n",
        "      D_z_loss, log_vars = mtl_D(z_gen, [D_fakes,D_fakes,D_fakes,D_fakes])\n",
        "      #Discriminator loss\n",
        "      D_loss = D_x_loss + D_z_loss \n",
        "      mtl_D.zero_grad()\n",
        "      D_loss.backward()\n",
        "      mtl_D_opt.step()\n",
        "      ### Train Generator\n",
        "      z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "      z_gen = G(z)\n",
        "      z_outputs, mi_z_outputs, mi_z_d1_outputs, mi_z_d2_outputs = D(z_gen)\n",
        "      G_z_loss = criterion(z_outputs, D_labels)\n",
        "      #Generator loss\n",
        "      G_loss = G_z_loss \n",
        "      G.zero_grad()\n",
        "      G_loss.backward()\n",
        "      G_opt.step()                  \n",
        "      step = step + 1\n",
        "      #Save losses / uncertainty weights\n",
        "      loss_d.append(D_loss.item())\n",
        "      loss_g.append(G_loss.item())\n",
        "      log_var1.append(log_vars[0])\n",
        "      log_var2.append(log_vars[1])\n",
        "      log_var3.append(log_vars[2])\n",
        "      log_var4.append(log_vars[3])\n",
        "      #Print progress\n",
        "      if step % 250 == 0:\n",
        "        print('Epoch: [%d/%d] - G Loss: %f - D Loss: %f' % (e+1, num_epochs, G_loss.item(), D_loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtqHhWsat_7B"
      },
      "source": [
        "Plot losses and lambda throughout training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMvtlnoP1VR0"
      },
      "source": [
        "fig, ((ax1)) = plt.subplots(1, 1, figsize=(5, 3))\n",
        "ax1.plot(loss_g, \"orange\",alpha=.65)\n",
        "ax1.plot(loss_d, \"green\",alpha=.65)\n",
        "ax1.set_title(\"Losses\", fontsize=15, fontweight='bold')\n",
        "ax1.legend(('G Loss', 'D Loss'),loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}