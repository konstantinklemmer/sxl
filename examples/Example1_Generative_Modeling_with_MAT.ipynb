{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Example1_Generative_Modeling_with_MAT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gm2vebW3Y5RL"
      },
      "source": [
        "# Example 1: Generative Modeling with Moran's Auxiliary Task (MAT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57010mw88xKm",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Load required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYDZP7iivcYG",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import requests\n",
        "from urllib.request import urlretrieve\n",
        "import urllib.request, json \n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1iJHIax0UnG6"
      },
      "source": [
        "Check GPU device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gOXRv5W6UpKQ",
        "colab": {}
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yTAJRx7ueNwF"
      },
      "source": [
        "Helper functions, as found in `src/utils.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XQWa9wRfv1xt",
        "colab": {}
      },
      "source": [
        "#Normalize data values. Default is [0,1] range; if min_val = -1, range is [-1,1]\n",
        "def normal(tensor,min_val=-1):\n",
        "  t_min = torch.min(tensor)\n",
        "  t_max = torch.max(tensor)\n",
        "  if t_min == 0 and t_max == 0:\n",
        "    return torch.tensor(tensor)\n",
        "  if min_val == -1:\n",
        "    tensor_norm = 2 * ((tensor - t_min) / (t_max - t_min)) - 1\n",
        "  if min_val== 0:\n",
        "    tensor_norm = ((tensor - t_min) / (t_max - t_min))\n",
        "  return torch.tensor(tensor_norm)\n",
        "\n",
        "#Light-weight Local Moran's I for tensor data, requiring a sparse weight matrix input. \n",
        "#This can be used when there is no need to re-compute the weight matrix at each step\n",
        "def lw_tensor_local_moran(y,w_sparse,na_to_zero=True,norm=True,norm_min_val=-1):\n",
        "  y = y.reshape(-1)\n",
        "  n = len(y)\n",
        "  n_1 = n - 1\n",
        "  z = y - y.mean()\n",
        "  sy = y.std()\n",
        "  z /= sy\n",
        "  den = (z * z).sum()\n",
        "  zl = torch.tensor(w_sparse * z)\n",
        "  mi = n_1 * z * zl / den\n",
        "  if na_to_zero==True:\n",
        "    mi[torch.isnan(mi)] = 0\n",
        "  if norm==True:\n",
        "    mi = normal(mi,min_val=norm_min_val)\n",
        "  return torch.tensor(mi)\n",
        "\n",
        "#Batch version of lw_tensor_local_moran()\n",
        "#Computes the (normalized) local Moran's I for an input batch\n",
        "def batch_lw_tensor_local_moran(y_batch,w_sparse,na_to_zero=True,norm=True,norm_min_val=-1):\n",
        "  batch_size = y_batch.shape[0]\n",
        "  N = y_batch.shape[3]\n",
        "  mi_y_batch = torch.zeros(y_batch.shape)\n",
        "  for i in range(batch_size):\n",
        "    y = y_batch[i,:,:,:].reshape(N,N)\n",
        "    y = y.reshape(-1)\n",
        "    n = len(y)\n",
        "    n_1 = n - 1\n",
        "    z = y - y.mean()\n",
        "    sy = y.std()\n",
        "    z /= sy\n",
        "    den = (z * z).sum()\n",
        "    zl = torch.tensor(w_sparse * z)\n",
        "    mi = n_1 * z * zl / den\n",
        "    if na_to_zero==True:\n",
        "      mi[torch.isnan(mi)] = 0\n",
        "    if norm==True:\n",
        "      mi = normal(mi,min_val=norm_min_val)\n",
        "    mi_y_batch[i,0,:,:] = mi.reshape(N,N)\n",
        "  return mi_y_batch    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0N80J5ZKeybo"
      },
      "source": [
        "Load sparse spatial weight matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w7WvJrV9B5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_64.npz','w_sparse_64.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_32.npz','w_sparse_32.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_16.npz','w_sparse_16.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_8.npz','w_sparse_8.npz')\n",
        "urlretrieve('https://github.com/konstantinklemmer/sxl/raw/master/data/w/w_sparse_4.npz','w_sparse_4.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_iEDEtRUmzTb",
        "colab": {}
      },
      "source": [
        "w_sparse_64 = scipy.sparse.load_npz('w_sparse_64.npz')\n",
        "w_sparse_32 = scipy.sparse.load_npz('w_sparse_32.npz')\n",
        "w_sparse_16 = scipy.sparse.load_npz('w_sparse_16.npz')\n",
        "w_sparse_8 = scipy.sparse.load_npz('w_sparse_8.npz')\n",
        "w_sparse_4 = scipy.sparse.load_npz('w_sparse_4.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MDwt8YDd0IZz"
      },
      "source": [
        "## Data\n",
        "\n",
        "As customary for GAN training, data is normalized in the range `[-1,1]`. The local Moran's I of the data can be computed at this step already, to avoid further computational burden during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9UL7XzV6l7To"
      },
      "source": [
        "### (1) d1: Toy Example - Gaussian peak / dip (32x32)\n",
        "\n",
        "Generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kg-ckBwE0I4U",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "def random_peak_dip(X, Y, s=7): #For smaller peak, use s = [2,3], for larger peak use s = [5,6,...]\n",
        "    \n",
        "    #Alternate a and b between [-1,11] or [0,10] (if less edge cases are wanted)\n",
        "    a = torch.randperm(torch.arange(0,10).numel())[:1].float()\n",
        "    b = torch.randperm(torch.arange(0,10).numel())[:1].float()\n",
        "    \n",
        "    c = 10 - a\n",
        "    d = 10 - b\n",
        "    \n",
        "    term1 = .75*torch.exp(-((9*X - a).pow(2) + (9*Y - b).pow(2))/ s) \n",
        "    term2 = -(.75*torch.exp(-((9*X - c).pow(2) + (9*Y - d).pow(2))/ s))\n",
        "    \n",
        "    f= term1 + term2\n",
        "    \n",
        "    return f\n",
        "\n",
        "xv1, yv1 = torch.meshgrid([torch.linspace(0, 1, 32), torch.linspace(0, 1, 32)])\n",
        "train_x = torch.cat((\n",
        "    xv1.contiguous().view(xv1.numel(), 1), \n",
        "    yv1.contiguous().view(yv1.numel(), 1)),\n",
        "    dim=1\n",
        ")\n",
        "\n",
        "#Set seed for reproducibility\n",
        "torch.manual_seed(99)\n",
        "#Define number of data samples\n",
        "t = 7000\n",
        "#Add random noise to unnoisy data from Franke's function; different noise eacht step\n",
        "N = 32\n",
        "d1= torch.zeros(t,2,N,N)\n",
        "for i in range(t):\n",
        "    train_y_t = torch.zeros(32,32)\n",
        "    while len((train_y_t == 0).nonzero()) == N**2:\n",
        "        f = random_peak_dip(train_x[:, 0], train_x[:, 1])\n",
        "        f = torch.stack([f], -1).squeeze(1)\n",
        "        train_y_t = f\n",
        "        train_y_t = normal(torch.tensor(f).reshape(-1),min_val=-1)\n",
        "    d1[i,0,:,:] = train_y_t.reshape(32,32)\n",
        "d1[:,1,:,:] = batch_lw_tensor_local_moran(d1[:,0,:,:].reshape(t,1,N,N),w_sparse_32,norm_min_val=-1).reshape(t,N,N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d14VHz1xQzFh"
      },
      "source": [
        "### (2) d2: Petrel grid (32x32)\n",
        "\n",
        "Download and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "149_fUvcRFEy",
        "colab": {}
      },
      "source": [
        "with urllib.request.urlopen(\"https://github.com/konstantinklemmer/sxl/raw/master/data/list_petrel.json\") as url:\n",
        "    train_y = np.array(json.loads(url.read().decode()))\n",
        "\n",
        "N = 32\n",
        "t = train_y.shape[0]\n",
        "d2 = torch.zeros(t,2,N,N)\n",
        "for i in range(t-1):\n",
        "    train_y_t = torch.tensor(train_y[i,:,:])\n",
        "    train_y_t = torch.tensor(normal(train_y_t.reshape(-1)))\n",
        "    d2[i,0,:,:] = train_y_t.reshape(N,N)\n",
        "d2[:,1,:,:] = batch_lw_tensor_local_moran(d2[:,0,:,:].reshape(t,1,N,N),w_sparse_32,norm_min_val=-1).reshape(t,N,N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoD3zYq7SS2t"
      },
      "source": [
        "### (3) d3: DEM (32x32)\n",
        "\n",
        "Download and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mS8v0nfrSc3Y",
        "colab": {}
      },
      "source": [
        "with urllib.request.urlopen(\"https://github.com/konstantinklemmer/sxl/raw/master/data/list_dem.json\") as url:\n",
        "    train_y = np.array(json.loads(url.read().decode()))\n",
        "    \n",
        "N = 32\n",
        "t = train_y.shape[0]\n",
        "d3 = torch.zeros(t,2,N,N)\n",
        "for i in range(t-1):\n",
        "    train_y_t = torch.tensor(train_y[i,:,:])\n",
        "    train_y_t = torch.tensor(normal(train_y_t.reshape(-1)))\n",
        "    d3[i,0,:,:] = train_y_t.reshape(N,N)\n",
        "d3[:,1,:,:] = batch_lw_tensor_local_moran(d3[:,0,:,:].reshape(t,1,N,N),w_sparse_32,norm_min_val=-1).reshape(t,N,N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CGO6ONMbS5be"
      },
      "source": [
        "### (4) d4: Tree canopy (64x64)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7EPGtng1TRQ5",
        "colab": {}
      },
      "source": [
        "with urllib.request.urlopen(\"https://github.com/konstantinklemmer/sxl/raw/master/data/list_tree.json\") as url:\n",
        "    train_y = np.array(json.loads(url.read().decode()))\n",
        "    \n",
        "N = 64\n",
        "t = train_y.shape[0]\n",
        "d4 = torch.zeros(t,2,N,N)\n",
        "for i in range(t-1):\n",
        "    train_y_t = torch.tensor(train_y[i,:,:])\n",
        "    train_y_t = torch.tensor(StandardScaler().fit_transform(train_y_t.reshape(-1,1)))\n",
        "    train_y_t = torch.tensor(normal(train_y_t.reshape(-1)))\n",
        "    d4[i,0,:,:] = train_y_t.reshape(N,N)\n",
        "d4[:,1,:,:] = batch_lw_tensor_local_moran(d4[:,0,:,:].reshape(t,1,N,N),w_sparse_64,norm_min_val=-1).reshape(t,N,N)\n",
        "d4 = d4[:1800,:,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4HuPIwhh5m1"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kE4gSMROikB4"
      },
      "source": [
        "Define the model architectures for Discriminator (**D**) and Generator (**G**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fmGUL84Q1fWC",
        "colab": {}
      },
      "source": [
        "###\n",
        "# VANILLA GAN\n",
        "###\n",
        "\n",
        "class Discriminator_VanillaGAN_MAT(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Discriminator w/ MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=N*N, num_classes=1):\n",
        "        super(Discriminator_VanillaGAN_MAT, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(\n",
        "            nn.Linear(256, num_classes),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.output_t2 = nn.Sequential(\n",
        "            nn.Linear(256, num_classes),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        if N==32:\n",
        "          mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_32)\n",
        "        if N==64:\n",
        "          mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_64)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        y_ = x.view(x.size(0), -1)\n",
        "        y_ = self.layer(y_)\n",
        "        mi_x = mi_x.view(mi_x.size(0), -1)\n",
        "        mi_y_ = self.layer(mi_x)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        return y_, mi_y_\n",
        "\n",
        "class Generator_VanillaGAN(nn.Module):\n",
        "    \"\"\"\n",
        "        Simple Generator w/ MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=100, num_classes=N*N):\n",
        "        super(Generator_VanillaGAN, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, num_classes),\n",
        "            nn.Tanh()\n",
        "        )      \n",
        "    def forward(self, x):\n",
        "        y_ = self.layer(x)\n",
        "        y_ = y_.view(x.size(0), 1, N, N)\n",
        "        return y_\n",
        "\n",
        "###\n",
        "# DCGAN \n",
        "###\n",
        "\n",
        "class Discriminator_DCGAN_MAT_32(nn.Module):\n",
        "    \"\"\"\n",
        "        DeepConv Discriminator\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channel=1, num_classes=1):\n",
        "        super(Discriminator_DCGAN_MAT_32, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, 512, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 256, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AvgPool2d(4)\n",
        "        )\n",
        "        self.output_t2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AvgPool2d(4)\n",
        "        )\n",
        "        self.fc_t1 = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.fc_t2 = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_32)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        y_ = self.conv(x)\n",
        "        mi_y_ = self.conv(mi_x)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        y_ = y_.view(y_.size(0), -1)\n",
        "        mi_y_ = mi_y_.view(mi_y_.size(0), -1)\n",
        "        y_ = self.fc_t1(y_)\n",
        "        mi_y_ = self.fc_t2(mi_y_)\n",
        "        return y_, mi_y_\n",
        "\n",
        "class Generator_DCGAN_32(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Generator\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=100, out_channel=1):\n",
        "        super(Generator_DCGAN_32, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 4*4*512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, out_channel, 4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )      \n",
        "    def forward(self, x, y=None):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_ = self.fc(x)\n",
        "        y_ = y_.view(y_.size(0), 512, 4, 4)\n",
        "        y_ = self.conv(y_)\n",
        "        return y_\n",
        "\n",
        "class Discriminator_DCGAN_MAT_64(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Discriminator\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channel=1, num_classes=1):\n",
        "        super(Discriminator_DCGAN_MAT_64, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, 512, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(512, 256, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 128, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )        \n",
        "        self.output_t2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )          \n",
        "        self.fc_t1 = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.fc_t2 = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_64)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        y_ = self.conv(x)\n",
        "        mi_y_ = self.conv(mi_x)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        y_ = y_.view(y_.size(0), -1)\n",
        "        mi_y_ = mi_y_.view(mi_y_.size(0), -1)\n",
        "        y_ = self.fc_t1(y_)\n",
        "        mi_y_ = self.fc_t2(mi_y_)\n",
        "        return y_, mi_y_\n",
        "\n",
        "class Generator_DCGAN_64(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Generator\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channel=1, input_size=100):\n",
        "        super(Generator_DCGAN_64, self).__init__()\n",
        "        assert IMAGE_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (IMAGE_DIM[0] // 2**4, IMAGE_DIM[1] // 2**4)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, self.init_dim[0]*self.init_dim[1]*512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, out_channel, 4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_ = self.fc(x)\n",
        "        y_ = y_.view(y_.size(0), 512, self.init_dim[0], self.init_dim[1])\n",
        "        y_ = self.conv(y_)\n",
        "        return y_\n",
        "\n",
        "###\n",
        "# EDGAN\n",
        "###\n",
        "\n",
        "class Discriminator_EDGAN_MAT_32(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Discriminator\n",
        "    \"\"\"\n",
        "    def __init__(self,nc=1,ndf1=32):\n",
        "        super(Discriminator_EDGAN_MAT_32,self).__init__()\n",
        "        self.conv = nn.Sequential(nn.Conv2d(nc,ndf1,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf1),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ndf1,ndf1*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf1*2),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ndf1*2,ndf1*4,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf1*4),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(nn.Conv2d(ndf1*4,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "        self.output_t2 = nn.Sequential(nn.Conv2d(ndf1*4,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x, y=None):\n",
        "        mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_32)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        y_ = self.conv(x)\n",
        "        mi_y_ = self.conv(mi_x)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        y_ = y_.view(y_.size(0), -1)\n",
        "        mi_y_ = mi_y_.view(mi_y_.size(0), -1)\n",
        "        return y_, mi_y_\n",
        "\n",
        "class Generator_EDGAN_32(nn.Module):\n",
        "    \"\"\"\n",
        "        Encoder-Decoder Generator\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=100, nc=1, ngf=N):\n",
        "        super(Generator_EDGAN_32, self).__init__()\n",
        "        assert IMAGE_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (IMAGE_DIM[0] // 2**4, IMAGE_DIM[1] // 2**4)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, N*N),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.encoder = nn.Sequential(nn.Conv2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ngf*2,ngf*4,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*4),\n",
        "          nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(nn.ConvTranspose2d(ngf*4,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf*2,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )   \n",
        "    def forward(self, x, y=None):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_ = self.fc(x)\n",
        "        y_ = y_.view(y_.size(0), 1, N, N)\n",
        "        y_ = self.encoder(y_)\n",
        "        y_ = self.decoder(y_)\n",
        "        return y_\n",
        "\n",
        "class Discriminator_EDGAN_MAT_64(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Discriminator \n",
        "    \"\"\"\n",
        "    def __init__(self,nc=1,ndf=64):\n",
        "        super(Discriminator_EDGAN_MAT_64,self).__init__()\n",
        "        self.conv = nn.Sequential(nn.Conv2d(nc,ndf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ndf,ndf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf*2),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ndf*2,ndf*4,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf*4),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ndf*4,ndf*8,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ndf*8),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "        )\n",
        "        self.output_t1 = nn.Sequential(nn.Conv2d(ndf*8,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        )   \n",
        "        self.output_t2 = nn.Sequential(nn.Conv2d(ndf*8,1,kernel_size=4,stride=1,padding=0),\n",
        "          nn.Sigmoid()\n",
        "        ) \n",
        "    def forward(self, x, y=None):\n",
        "        mi_x = batch_lw_tensor_local_moran(x.detach().cpu(),w_sparse_64)\n",
        "        mi_x = mi_x.to(DEVICE)\n",
        "        y_ = self.conv(x)\n",
        "        mi_y_ = self.conv(mi_x)\n",
        "        y_ = self.output_t1(y_)\n",
        "        mi_y_ = self.output_t2(mi_y_)\n",
        "        y_ = y_.view(y_.size(0), -1)\n",
        "        mi_y_ = mi_y_.view(mi_y_.size(0), -1)\n",
        "        return y_, mi_y_\n",
        "\n",
        "class Generator_EDGAN_64(nn.Module):\n",
        "    \"\"\"\n",
        "        Encoder-Decoder Generator\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=100, nc=1, ngf=N):\n",
        "        super(Generator_EDGAN_64, self).__init__()\n",
        "        assert IMAGE_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (IMAGE_DIM[0] // 2**4, IMAGE_DIM[1] // 2**4)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, N*N),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.encoder = nn.Sequential(nn.Conv2d(nc,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.LeakyReLU(0.2,inplace=True),\n",
        "          nn.Conv2d(ngf*2,ngf*4,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*4),\n",
        "          nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(nn.ConvTranspose2d(ngf*4,ngf*2,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf*2),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf*2,ngf,kernel_size=4,stride=2,padding=1),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n",
        "          nn.Tanh()\n",
        "        )    \n",
        "    def forward(self, x, y=None):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_ = self.fc(x)\n",
        "        y_ = y_.view(y_.size(0), 1, N, N)\n",
        "        y_ = self.encoder(y_)\n",
        "        y_ = self.decoder(y_)\n",
        "        return y_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yhl3VFqmrF_r"
      },
      "source": [
        "Define training configuration:\n",
        "\n",
        "- `experiment`: Dataset (Toy=\"d1\", PetrelGrid=\"d2\", DEM=\"d3\", TreeCanopy=\"d4)\n",
        "- `train_split`: % of data to use for training (in case held-out data is needed for evaluation)\n",
        "- `model`: MAT augmented GAN model to-be-used; models abailable are a VanillaGAN, DCGAN and EDGAN\n",
        "- `batch_size`: training batch size\n",
        "- `lambda_`: auxiliary task loss weight parameter. We experimented (and got good results) with values `[0.01, 0.1, 1]`\n",
        "- `loss_method`: using a normal loss(=\"N\") or Wasserstein loss (=\"W\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HuEewWsTrJgq",
        "colab": {}
      },
      "source": [
        "from decimal import Decimal, getcontext\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "getcontext().prec = 3\n",
        "torch.manual_seed(99)\n",
        "\n",
        "### DEFINE EXPERIMENT SETTINGS ###\n",
        "experiment = \"d2\" \n",
        "train_split = Decimal(0.8) # 80% training data\n",
        "model = \"EDGAN_MAT\" # chose from [\"VanillaGAN_MAT\",\"DCGAN_MAT\",\"EDGAN_MAT\"]\n",
        "batch_size = 32 # define batch size\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Train on GPU or CPU\n",
        "lambda_ = 0.1 # weight of the Moran's Auxiliary Task loss\n",
        "loss_method = \"N\" # loss method to be used; chose normal (=\"N\") or Wasserstein (=\"W\")\n",
        "###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n8In3NBI9OFJ"
      },
      "source": [
        "Define training loop and run the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cn7WIaI99T-W",
        "colab": {}
      },
      "source": [
        "##PREPARATION\n",
        "# Prepare input\n",
        "data = eval(experiment)\n",
        "test_split = Decimal(1 - train_split)\n",
        "n = data.shape[0]\n",
        "N = data.shape[3]\n",
        "IMAGE_DIM = (N,N,1)\n",
        "train_set, val_set = torch.utils.data.random_split(data, [int(n * train_split), int(n * test_split)])\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "test_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
        "# Set training epochs\n",
        "if experiment==\"d1\":\n",
        "  num_epochs = 40\n",
        "if experiment==\"d2\":\n",
        "  num_epochs = 300\n",
        "if experiment==\"d3\":\n",
        "  num_epochs = 100\n",
        "if experiment==\"d4\":\n",
        "  num_epochs = 100\n",
        "# Model\n",
        "if model==\"VanillaGAN_MAT\":\n",
        "  D = Discriminator_VanillaGAN_MAT().to(DEVICE)\n",
        "  G = Generator_VanillaGAN().to(DEVICE)\n",
        "if model==\"DCGAN_MAT\":\n",
        "  if N==32:\n",
        "    D = Discriminator_DCGAN_MAT_32().to(DEVICE)\n",
        "    G = Generator_DCGAN_32().to(DEVICE)\n",
        "  if N==64:\n",
        "    D = Discriminator_DCGAN_MAT_64().to(DEVICE)\n",
        "    G = Generator_DCGAN_64().to(DEVICE)\n",
        "if model==\"EDGAN_MAT\":\n",
        "  if N==32:\n",
        "    D = Discriminator_EDGAN_MAT_32().to(DEVICE)\n",
        "    G = Generator_EDGAN_32().to(DEVICE)\n",
        "  if N==64:\n",
        "    D = Discriminator_EDGAN_MAT_64().to(DEVICE)\n",
        "    G = Generator_EDGAN_64().to(DEVICE)\n",
        "# Labels\n",
        "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator Label to real\n",
        "D_labels = D_labels - 0.1\n",
        "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label to fake\n",
        "# Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "D_opt = torch.optim.Adam(D.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "G_opt = torch.optim.Adam(G.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "# Other\n",
        "g_step = 0\n",
        "step = 0\n",
        "n_noise = 100\n",
        "### TRAINING\n",
        "for e in range(num_epochs):\n",
        "  # Within each iteration, we will go over each minibatch of data\n",
        "  for minibatch_i, (x_batch) in enumerate(train_loader):\n",
        "    # Get data\n",
        "    x = x_batch[:,0,:,:]\n",
        "    x = x.reshape(batch_size,1,N,N).to(DEVICE)\n",
        "    # Training Discriminator\n",
        "    x_outputs, mi_x_outputs = D(x)\n",
        "    z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "    z_gen = G(z)\n",
        "    z_outputs, mi_z_outputs = D(z_gen)\n",
        "    if loss_method==\"W\":\n",
        "      D_x_loss = torch.mean(x_outputs)\n",
        "      D_z_loss = torch.mean(z_outputs)\n",
        "      D_mi_x_loss = torch.mean(mi_x_outputs)\n",
        "      D_mi_z_loss = torch.mean(mi_z_outputs)\n",
        "      D_loss = (D_z_loss - D_x_loss) + lambda_ * (D_mi_z_loss - D_mi_x_loss)\n",
        "    else:\n",
        "      D_x_loss = criterion(x_outputs, D_labels)\n",
        "      D_z_loss = criterion(z_outputs, D_fakes)\n",
        "      D_mi_x_loss = criterion(mi_x_outputs, D_labels)\n",
        "      D_mi_z_loss = criterion(mi_z_outputs, D_fakes)\n",
        "      D_loss = D_x_loss + D_z_loss + lambda_ * (D_mi_x_loss + D_mi_z_loss)\n",
        "    D.zero_grad()\n",
        "    D_loss.backward()\n",
        "    D_opt.step()\n",
        "    if loss_method==\"W\":\n",
        "      for p in D.parameters():\n",
        "        p.data.clamp_(-0.01, 0.01)\n",
        "    #Train Generator\n",
        "    g_step += 1\n",
        "    z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "    z_gen = G(z)\n",
        "    z_outputs, mi_z_outputs = D(z_gen)\n",
        "    if loss_method==\"W\":\n",
        "      G_loss = -torch.mean(z_outputs)\n",
        "    else:\n",
        "      G_z_loss = criterion(z_outputs, D_labels)\n",
        "      G_loss = G_z_loss \n",
        "    G.zero_grad()\n",
        "    G_loss.backward()\n",
        "    G_opt.step()    \n",
        "    #Print progress:\n",
        "    if step % 100 == 0:\n",
        "      print(\"Epoch [%d/%d] - G Loss: %f - D Loss: %f\" % (e, num_epochs,G_loss.item(),D_loss.item()))              \n",
        "    #Increment steps\n",
        "    step = step + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DFlJy_O3iaTA"
      },
      "source": [
        "Print some example images from the final Generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yyZLhx_PNsIk",
        "colab": {}
      },
      "source": [
        "def get_sample_image(G, n_noise):\n",
        "    \"\"\"\n",
        "        save sample 25 images\n",
        "    \"\"\"\n",
        "    z = torch.randn(25, n_noise).to(DEVICE)\n",
        "    y_hat = G(z).view(25, N, N) \n",
        "    result = y_hat.cpu().data.numpy()\n",
        "    img = np.zeros([5*N, 5*N])\n",
        "    for j in range(5):\n",
        "        img[j*N:(j+1)*N] = np.concatenate([x for x in result[j*5:(j+1)*5]], axis=-1)\n",
        "    return img\n",
        "\n",
        "G.eval()\n",
        "plt.imshow(get_sample_image(G, n_noise))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}